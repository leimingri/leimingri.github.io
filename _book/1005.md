# 大数定律与中心极限定理

大数定律和中心极限定理是概率论中的比较深入与重要的理论结果。本章主要讨论如下两个问题：

<font size=4>**大数定律**</font>

在一定条件下,一列随机变量的算术平均值（按某种意义）收敛于所希望的平均值的定理称为“大数定律”。

* 大数定律为概率论所存在的基础——“概率是频率的稳定值”提供了理论依据，它以严格的数学形式表达了随机现象最根本的性质之一：
      平均结果的稳定性。它是随机现象统计规律的具体表现，也成为数理统计的理论基础。

<font size=4>**中心极限定理**</font>

在一定条件下，大量的相互独立的随机变量之和的概率分布近似于正态分布的定理称为“中心极限定理”。

* 中心极限定理它以严格的数学形式证明了随机现象另一个最根本的性质之一：如果一个量是由大量相互独立的随机因素的影响所造成，
      而每一个别因素在总影响中所起的作用不大，则这种量一般都服从或近似服从正态分布。它是随机现象统计规律的具体表现，也成为数理
      统计的理论基础。而正态分布又是应用最广的分布。

## 大数定理

### 大数定理

<font size=4>**（1）准备知识**</font>

**切比雪夫(Chebyshev)不等式**

设随机变量$X$的数学期望$E(X)=\mu$, $D(X)=\sigma^{2}<+\infty$，则对任意的正数$\varepsilon$，有

$$\begin{aligned}
P\{|X-\mu|\geq \varepsilon\}\leq \frac{\sigma^{2}}{\varepsilon^{2}}
\end{aligned} \tag{5.1}$$
其等价为
$$\begin{aligned}
P\{|X-\mu|< \varepsilon\}\geq 1-\frac{\sigma^{2}}{\varepsilon^{2}}
\end{aligned} \tag{5.2}$$

证明：（1）、设连续型随机变量$X$的概率密度函数为$f(x)$，则
$$\begin{aligned}
\begin{split}
P\{|X-\mu|\geq \varepsilon\}&=\int_{|x-\mu|\geq \varepsilon}f(x)dx\leq \int_{|x-\mu|\geq \varepsilon} \frac{(x-\mu)^{2}}{\varepsilon^{2}}f(x)dx\\
&\leq \int_{-\infty}^{+\infty} \frac{(x-\mu)^{2}}{\varepsilon^{2}}f(x)dx=\frac{D(X)}{\varepsilon^{2}}=\frac{\sigma^{2}}{\varepsilon^{2}}
\end{split}
\end{aligned} \tag{5.3}$$

（2）、设离散型随机变量$X$的概率分布为$p_{k}=P\{X=x_{k}\}, k=1,2,\cdots$，则
$$\begin{aligned}
\begin{split}
P\{|X-\mu|\geq \varepsilon\}&=\sum_{|x_{k}-\mu|\geq \varepsilon}p_{k}\leq \sum_{|x_{k}-\mu|\geq \varepsilon} \frac{(x_{k}-\mu)^{2}}{\varepsilon^{2}}p_{k}\\
&\leq \sum_{k}\frac{(x_{k}-\mu)^{2}}{\varepsilon^{2}}p_{k}=\frac{\sigma^{2}}{\varepsilon^{2}}
\end{split}
\end{aligned} \tag{5.4}$$

**说明：**

* 从定理中可以看出，如果$D(X)$越小，那么随机变量$X$取值于开区间$(E(X)-\varepsilon, E(X)+\varepsilon)$中的概率就越大，
      这也进一步说明方差是一个反映随机变量在其分布中心$E(X)$附近集中程度的数量指标，$D(X)$越小，$X$的取值越集中于$E(X)$附近。
      
* 利用Chebyshev不等式可以在随机变量的分布未知的情况下，利用它的期望和方差对其在某个范围内取值的概率作出估计，当然这个估计是比较保守的。


<font size=4>**（2）例题**</font>

**例1：**若某班某次考试的平均分为80分，标准差为10，试估计及格率至少为多少？

解：用随机变量X表示学生成绩，则数学期望$E(X)=80$，方差$D(X)=100$，所以
\begin{equation*}
\begin{split}
P\{60\leq X\leq 100\}&\geq P\{60<X<100\}\\
&=P\{|X-80|<20\}\\
&\geq 1-\frac{100}{20^{2}}=75\%
\end{split}
\end{equation*}
所以及格率至少为75\%．

**例2：**一电网有1万盏路灯，晚上每盏灯开的概率为0.7，求同时开的灯数在6800至7200之间的概率至少为多少？

解：设$X$为同时开的灯数，并且$X\sim B(10^{4}, 0.7)$，则有
$$P\{6800\leq X\leq 7200\}=\sum_{k=6800}^{7200}C_{10^{4}}^{k}\cdot 0.7^{k}\cdot 0.3^{10^{4}-k}.$$ 
计算起来相当麻烦。

由二项分布知
$$E(X)=np=7000, D(X)=np(1-p)=2100.$$ 
用切比雪夫不等式有
$$P\{6800\leq X\leq 7200\}=P\{|X-7000|\leq 200\}\geq 1-\frac{2100}{200^{2}}=0.95$$

**例3：**已知$n$重伯努利试验中参数$p=0.75$，问至少应做多少次试验，才能使试验成功的频率在0.74和0.76之间的概率不低于0.90？

解：设需做$n$次试验，其中成功的次数为$X$，则$X\sim B(n, p)$，且有
$$E(X)=np, D(X)=np(1-p).$$ 
根据契比谢夫不等式应有
\begin{equation*}
\begin{split}
P\{0.74<\frac{X}{n}<0.76\}&=P\{|\frac{X}{n}-0.75|<0.01\}\\
&\geq 1-\frac{\frac{1}{n^{2}}np(1-p)}{0.01^{2}}\geq 0.9
\end{split}
\end{equation*} 
解得
$$n\geq \frac{p(1-p)}{0.1\times 0.01^{2}}=\frac{0.75\times(1-0.75)}{0.1\times 0.01^{2}}=18750,$$
所以至少应做18750次试验。

<font size=4>**（3）大数定律**</font>

对某个随机变量X进行大量的重复观测，所得到的大批观测数据的算术平均值也具有稳定性，由于这类稳定性都是
在对随机变量进行大量重复试验的条件下呈现出来的，历史上把这种试验次数很大时出现的规律统称为大数定律。

在介绍大数定律之前，我们引入依概率收敛定义。

**定义：依概率收敛**

设随机变量序列$X_{1}$, $X_{2}$, $\cdots$, $X_{n}$, 如果存在常数$A$，使得对于任意的$\varepsilon>0$，有
$$\lim_{n\rightarrow\infty}P\{|X_{n}-A|< \varepsilon\}=1\tag{5.5}$$
则称随机变量序列$X_{1}$, $X_{2}$, $\cdots$, $X_{n}$依概率收敛于$A$，记为$X_{n}\longrightarrow{P}A$

* 当$n$无限增大时，对于任意的$\varepsilon>0$，事件$\{|X_{n}-A|< \varepsilon\}$发生的概率无限接近于1。

<font size=4>**（4）Chebyshev大数定律**</font>

**定理：Chebyshev大数定律**

设$X_{1}$，$X_{2}$，$\cdots$， $X_{n}$是相互独立的随机变量序列，$E(X_{i})<\infty, i=1,2,\cdots, n$， 且它们都有有限的方差，并且方差有共同的上界，即$D(X_{i})\leq K, i=1, 2, \cdots, n$，则对于任意的$\varepsilon>0$，有
$$\lim_{n\rightarrow\infty}P\left\{\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\frac{1}{n}\sum_{i=1}^{n}E(X_{i})\right|<\varepsilon\right\}=1 \tag{5.6}$$

证明：由条件可知
$$E\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)=\frac{1}{n}\sum_{i=1}^{n}E(X_{i}),\tag{5.7}$$ 
$$D\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)=\frac{1}{n^{2}}\sum_{i=1}^{n}D(X_{i})\leq \frac{1}{n^{2}}\cdot nK=\frac{K}{n}\tag{5.8}$$


所以由Chebyshev不等式，对于任意的$\varepsilon>0$，有
$$P\left\{\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\frac{1}{n}\sum_{i=1}^{n}E(X_{i})\right|<\varepsilon\right\}\geq 1-\frac{1}{\varepsilon^{2}}D\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}\right)\geq 1-\frac{K}{n\varepsilon^{2}}\tag{5.9}$$ 
又因为任何事件的概率都不超过1，则有
$$1-\frac{K}{n\varepsilon^{2}}\leq P\left\{\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\frac{1}{n}\sum_{i=1}^{n}E(X_{i})\right|<\varepsilon\right\}\leq 1\tag{5.10}$$ 
利用极限的夹逼准则，即得
$$\lim_{n\rightarrow\infty}P\left\{\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\frac{1}{n}\sum_{i=1}^{n}E(X_{i})\right|<\varepsilon\right\}=1\tag{5.11}$$

<font size=4>**（5）说明及推论**</font>

切比雪夫大数定律表明：独立随机变量序列$\{X_{n}\}$，如果方差有共同的上界，则该序列的算术平均值$\frac{1}{n}\sum_{i=1}^{n}X_{i}$与
其数学期望的平均值$\frac{1}{n}\sum_{i=1}^{n}E(X_{i})$的偏差是很小的。即当$n$充分大的时候，$\frac{1}{n}\sum_{i=1}^{n}X_{i}$差不多不再是随机的了，
取值接近于其数学期望的概率接近于1。切比雪夫大数定律给出了平均值稳定性的科学描述。

**推论：独立同分布大数定律**

设$X_{1}$, $X_{2}$, $\cdots$, $X_{n}$是相互独立的随机变量序列，它们具有相同的数学期望与方差： $E(X_{k})=\mu$, $D(X_{k})=\sigma^{2}, k=1,2,\cdots$. 作前$n$个随机变量的算术平均值：$\overline{X}_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$，则对于任意的$\varepsilon>0$，有
$$\lim_{n\rightarrow\infty}P\left\{\left|\overline{X}_{n}-\mu\right|<\varepsilon\right\}=\lim_{n\rightarrow\infty}P\left\{\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\mu\right|<\varepsilon\right\}=1\tag{5.12}$$

上述推论是Chebyshev大数定律中当数学期望和方差给定具体值时的特殊情况。它表明当$n$很大时，随机变量$X_{1}$, $X_{2}$, $\cdots$, $X_{n}$ 的算术平均值在概率意义下接近于数学期望$E(X_{k})=\mu$.其作用是：在数理统计中如不知道$\mu$，则可用
其算术平均值来近似代替，则上述推论就提供了理论依据。

**定理：伯努利大数定律**

设$n_{A}$是$n$重伯努利试验中事件$A$发生的次数，$p (0<P<1)$是事件$A$在每次试验中发生的概率，则对于任意正数$\varepsilon$，有
$$\lim_{n\rightarrow\infty}P\left\{\left|\frac{n_{A}}{n}-p\right|<\varepsilon\right\}=1\tag{5.13}$$

证明：引入相互独立的随机变量$X_{i}, i=1,2,\cdots, n$，
\begin{equation*}
X_{i}=\left\{
        \begin{array}{ll}
          0, & \text{第$i$试验$A$不发生;} \\
          1, & \text{第$i$试验$A$发生.}
        \end{array}
      \right.
\end{equation*} 
则$n_{A}=X_{1}+X_{2}+\cdots+X_{n}$.

由已知可知，$X_{i}, i=1,2,\cdots, n$相互独立且均服从参数为$p$的0-1分布，则有
$$E(X_{i})=p, D(X_{i})=p(1-p), i=1,2,\cdots,n$$ 
由独立同分布大数定律可知，对于任意正数$\varepsilon$
$$\lim_{n\rightarrow\infty}P\left\{\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-p\right|<\varepsilon\right\}=1\tag{5.14}$$

即
$$\lim_{n\rightarrow\infty}P\left\{\left|\frac{n_{A}}{n}-p\right|<\varepsilon\right\}=1\tag{5.15}$$

**说明：**

伯努利大数定律表明：事件$A$发生的频率$n_{A}/n$当$n$很大时依概率收敛于事件$A$发生的概率$p$。

也就是说对于任意正数$\varepsilon$，只要重复独立试验的次数$n$充分大，事件$\left\{\left|\frac{n_{A}}{n}-p\right|<\varepsilon\right\}$ 实际上几乎是必定要发生的，即对于给定的任意小的正数$\varepsilon$，在$n$充分大时，事件“$n_{A}/n$与概率$p$的偏差小于$\varepsilon$”
实际上几乎是必定要发生的。这也正是在大量重复独立试验中，频率$n_{A}/n$接近于概率$p$的真正含义，也就是我们所说的频率稳定性的真正含义。
所以当试验次数很大时，就可以利用事件发生的频率来近似地代替事件发生的概率。

<font size=4>**（6）辛钦大数定律**</font>

**定理：辛钦大数定律**

设$X_{1}$, $X_{2}$, $\cdots$， $X_{n}$相互独立，并服从同一分布，且具有相同的数学期望$E(X_{k})=\mu, k=1,2,\cdots$，则
$$\lim_{n\rightarrow\infty}P\left\{\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}-\mu\right|<\varepsilon\right\}=1\tag{5.16}$$

* 辛钦大数定律中，随机变量的方差可以不存在，只要独立同分布就可以了。

* 伯努利大数定律是辛钦定理的特殊情况。

* 辛钦大数定律提供了求随机变量数学期望$E(X)$的近似值的方法。这一思想方法将被应用于第七章中讲述的参数的点估计理论中，
      辛钦大数定律是数理统计部分中点估计理论的重要依据。

**辛钦大数定律的推广**

* **定理：高阶矩情形**
设$X_{1}$, $X_{2}$, $\cdots$, $X_{n}$相互独立，并服从同一分布，且具有相同的$k$阶原点矩$E(X_{i}^{k})=\mu_{k}, i=1,2,\cdots, k>1$，则有

$$\lim_{n\rightarrow\infty}P\left\{\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}-\mu_{k}\right|<\varepsilon\right\}=1\tag{5.17}$$

证明：因为$X_{1}$，$X_{2}$，$\cdots$， $X_{n}$相互独立，并服从同一分布，所以$X_{1}^{k}$， $X_{2}^{k}$，$\cdots$，$X_{n}^{k}$， $k>1$相互独立，并服从同一分布，且$E(X_{i}^{k})=\mu_{k}$存在，则由辛钦大数定律可知
$$\lim_{n\rightarrow\infty}P\left\{\left|\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}-\mu_{k}\right|<\varepsilon\right\}=1\tag{5.18}$$

<font size=4>**（7）补充说明**</font>

本章所考虑的大数定律均为弱大数定律，是建立在依概率收敛的基础上的。

* Chebyshev大数定律
* 独立同分布大数定律
* 伯努利大数定律
* 辛钦大数定律

与之对应的还有强大数定律，是建立在依概率1收敛的基础上的。

* 伯雷尔强大数定律
* 科尔莫戈罗夫强大数定律
* 独立同分布场合的强大数定律

## 中心极限定理

### 中心极限定理

<font size=4>**（1）引例**</font>

误差分析是人们经常遇到且感兴趣的随机变量，大量的研究表明，误差是由大量微小的相互独立的随机因素叠加而成的。现在考虑一位操作工在机床上加工机械轴，
要求其直径应符合规定要求，但加工后的机械轴与规定要求总会有一定误差，这是因为在加工时受到一些随机因素的影响，它们是：

* 在机床方面有机床振动与转速的影响；

* 在刀具方面有装配与磨损的影响；

* 在材料方面有钢材的成分、产地的影响；

* 在操作者方面有注意力集中程度、当天的情绪的影响；

* 在测量方面有度量工具误差、测量技术的影响；

* 在环境方面有车间温度、湿度、照明、工作电压的影响；

* 在具体场合还可列出许多其他影响因素。

由于这些因素很多，每个因素对加工精度的影响都是很微小的，而且每个因素的出现又都是人们无法控制的、随机的、时有时无、时正时负的。
这些因素的综合影响最终使每个机械轴的直径产生误差，若将这个误差记为$Y_{n}$，那么$Y_{n}$是随机变量，且可以将$Y_{n}$看作很多微小的随机波动$X_{1}$，
$X_{2}$，$\cdots$，$X_{n}$之和，即$Y_{n} = X_{1}+X_{2}+\cdots+X_{n}$，这里$n$是很大的，那么我们关心的是，当时，$Y_{n}$的分布是什么？

* 我们可以考虑用卷积公式去计算$Y_{n}$的分布，但这样的计算是相当复杂的、不现实的，而且也是不易实现的。
      有时即使能写出$Y_{n}$的分布，但由于其形式过于复杂而无法使用。

<font size=4>**（2）问题提出**</font>

在概率论中，我们已经知道正态分布居于头等重要的地位，许多随机变量都遵循
正态分布。自从高斯指出测量误差服从正态分布之后，人们发现，正态分布在自然
界中极为常见。并且大量实验观察也表明如果一个量是由大量相互独立的随机因素的影响所造成，
而每一个别因素在总影响中所起的作用不大， 则这种量一般都服从或近似服从正态分布。

这仅仅是经验之谈呢，还是确有理论依据呢？对于这样一个重要问题，在长达两个世纪内一直成为概率论研究的中心问题。

* 数学家们经过卓越工作建立了一系列定理，解决了这一问题，建立了一系列中心极限定理——是概率论中讨论随机变量和的分布以正态分布为极限的一组定理。

<font size=4>**（3）中心极限定理的发展历史**</font>

* 这个定理的第一版被法国数学家棣莫弗发现，他在1733年发表的卓越论文中使用正态分布去估计大量抛掷硬币出现正面次数的分布。这个超越时代的成果险些被历史遗忘。

* 著名法国数学家拉普拉斯在1812年发表的巨著 Th$\acute{e}$orie Analytique des Probabilit$\acute{e}$s中拯救了这个默默无名的理论.
      拉普拉斯扩展了棣莫弗的理论，指出二项分布可用正态分布逼近。但同棣莫弗一样，拉普拉斯的发现在当时并未引起很大反响。 
      
* 直到十九世纪末中心极限定理的重要性才被世人所知。1901年，俄国数学家里雅普诺夫用更普通的随机变量定义中心极限定理并在数学上进行了精确的证明。

<font size=4>**（4）常用的中心极限定理**</font>

**独立同分布的中心极限定理**

设$X_{1}$, $X_{2}$, $\cdots$, $X_{n}$, $\cdots$相互独立，并服从同一分布，且具有数学期望和方差：$E(X_{k})=\mu$，$D(X_{k})=\sigma^{2}>0$，$k=1,2,\cdots$，则随机变量之和$\sum_{k=1}^{n}X_{k}$的标准化变量
$$Y_{n}=\frac{\sum_{k=1}^{n}X_{k}-E\left(\sum_{k=1}^{n}X_{k}\right)}{\sqrt{D\left(\sum_{k=1}^{n}X_{k}\right)}}=\frac{\sum_{k=1}^{n}X_{k}-n\mu}{\sqrt{n}\sigma}\tag{5.19}$$
的分布函数$F_{n}(x)$对于任意的$x$满足 $$\lim_{n\rightarrow\infty}F_{n}(x)=\lim_{n\rightarrow\infty}P\left\{\frac{\sum_{k=1}^{n}X_{k}-n\mu}{\sqrt{n}\sigma}\leq x\right\}=\int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}}e^{-\frac{t^{2}}{2}}dt\tag{5.20}$$

* 该定理我们通常称之为林德伯格-勒维(Lindeberg-Levy)定理，该定理是这两位学者在上世纪20年代证明的，这里证明从略。

**说明：**

* 林德伯格-勒维定理表明，当$n$充分大时，$n$个具有期望和方差的独立同分布的随机变量之和近似服从正态分布。虽然在一般情况下，
      很难求出$X_{1}+X_{2}+\cdots+X_{n}$的分布的确切形式，但当$n$很大时，可以求出其近似分布。  
      
* 同样定理也表明了正态分布在概率论中的特殊地位：尽管$X_{1}$，$X_{2}$，$\cdots$，$X_{n}$， $\cdots$是未知的，只要$n$充分大时，其样本均值$\frac{1}{n}\sum_{k=1}^{n}X_{k}$的分布却是近似服从正态分布的：

$$\frac{\sum_{k=1}^{n}X_{k}-n\mu}{\sqrt{n}\sigma}=\frac{\frac{1}{n}\sum_{k=1}^{n}X_{k}-\mu}{\sigma/\sqrt{n}}=\frac{\overline{X}-\mu}{\sigma/\sqrt{n}}\sim N(0,1)\tag{5.21}$$
 这一结果是数理统计中大样本统计推断的基础

**例1：**某人要测量甲、乙两地之间的距离。限于测量工具，他分成 1200 段来测量。段测量误差（单位：厘米）服从于（-0.5,0.5)上的均匀分布。求总距离误差的绝对值超过20厘米的概率。

解：设第$k$段的测量误差为$X_{k}\sim U(-0.5, 0.5), k=1,2,\cdots,1200$, 且是独立同分布的随机变量，故有
$$E(X_{k})=0, D(X_{k})=\frac{1}{12}.$$
因此累积误差$\sum_{k=1}^{n}X_{k}$近似服从$N(0, 100)$，则所求概率为：
\begin{equation*}
\begin{split}
P\left\{\left|\sum_{k=1}^{n}X_{k}\right|\geq 20\right\}&=P\left\{\frac{\left|\sum_{k=1}^{n}X_{k}-0\right|}{\sqrt{1200/12}}\geq \frac{20}{\sqrt{1200/12}}\right\}\\
&=1-P\left\{\frac{\left|\sum_{k=1}^{n}X_{k}-0\right|}{10}\leq 2\right\}\\
&\approx 1-[\Phi(2)-\Phi(-2)]=0.0456
\end{split}
\end{equation*}

**李雅普诺夫中心极限定理**

设$X_{1}$，$X_{2}$，$\cdots$，$X_{n}$， $\cdots$相互独立，并且具有数学期望和方差：$E(X_{k})=\mu_{k}$，$D(X_{k})=\sigma_{k}^{2}>0$，$k=1,2,\cdots$，
记$B_{n}^{2}=\sum_{k=1}^{n}\sigma_{k}^{2}$，若存在$\delta>0$，使得当$n\rightarrow\infty$时$$\frac{1}{B_{n}^{2+\delta}}\sum_{k=1}^{n}E[|X_{k}-\mu_{k}|^{2+\delta}]\rightarrow 0\tag{5.22}$$
则随机变量之和$\sum_{k=1}^{n}X_{k}$的标准化变量 $$Z_{n}=\frac{\sum_{k=1}^{n}X_{k}-E\left(\sum_{k=1}^{n}X_{k}\right)}{\sqrt{D\left(\sum_{k=1}^{n}X_{k}\right)}}=\frac{\sum_{k=1}^{n}X_{k}-\sum_{k=1}^{n}\mu_{k}}{B_{n}}\tag{5.23}$$
的分布函数$F_{n}(z)$对于任意的$z$满足 $$\lim_{n\rightarrow\infty}F_{n}(z)=\lim_{n\rightarrow\infty}P\left\{\frac{\sum_{k=1}^{n}X_{k}-\sum_{k=1}^{n}\mu_{k}}{\sqrt{n}\sigma}\leq z\right\}=\int_{-\infty}^{z}\frac{1}{\sqrt{2\pi}}e^{-\frac{t^{2}}{2}}dt\tag{5.24}$$

<font size=4>**（5）独立同分布的中心极限定理特殊情况**</font>

**棣莫佛-拉普拉斯(De Moivre-Laplace)定理**

设随机变量$\eta_{n}, n=1,2,\cdots$相互独立且服从参数为$n, p (0<p<1)$的二项分布，则对于任意$x$，有
$$\lim_{n\rightarrow\infty}P\left\{\frac{\eta_{n}-np}{\sqrt{np(1-p)}}\leq x\right\}=\int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}}e^{-\frac{t^{2}}{2}}dt\tag{5.25}$$

* 定理表明：正态分布是二项分布的极限分布，当$n$充分大时可以用正态分布来计算二项分布的概率。

* 在第二章中已介绍当$n\rightarrow\infty$时，二项分布以泊松分布为极限分布；而在本章中二项分布又以正态分布为极限分布。区别是：

  * 在泊松定理中要求$np\rightarrow\lambda$， 如果$n$很大但$np$不大，那么应该用泊松定理去近似
  * 在中心极限定理中要求$np\rightarrow\infty$， 如果$n$，$np$都较大，那么应该用中心极限定理去近似。
 
**例2：**某单位有200台电话分机，每台分机有5\%的时间要使用外线通话。假定每台分机是否使用外线是相互独立的，问该单位总机要安装多少条外线，
才能以90\%以上的概率保证分机用外线时不等待？

解：设有$X$部分机同时使用外线，则有$X\sim B(200, 0.05)$，且
$$E(X)=np=10, \sqrt{np(1-p)}\approx 3.08$$
设有$N$条外线。由题意有$P\{X\leq N\}\geq 0.9$ 
由德莫佛-拉普拉斯定理得
\begin{equation*}
\begin{split}
P\{X\leq N\}&=P\left\{\frac{X-np}{\sqrt{np(1-p)}}\leq \frac{N-np}{\sqrt{np(1-p)}}\right\}\\
&=\Phi\left(\frac{N-np}{\sqrt{np(1-p)}}\right)=\Phi\left(\frac{N-10}{3.08}\right)\geq 0.9=\Phi(1.28)
\end{split}
\end{equation*}
故$N$应满足条件$\frac{N-10}{3.08}\geq 1.28$，得$N\geq 13.94$。故至少需要14条外线。

**例3：**在人寿保险公司里，有16000名同一年龄的人参加人寿保险。一年里这些人的死亡率为0.1\%；参加保险的人在一年的第一天交付保险费3元，
死亡时家属可以从保险公司领取2000元。 求

* 保险公司因开展这项业务获利不少于10000元的概率；

* 保险公司因开展这项业务亏本的概率。

解：由题意，死亡人数$X\sim B(16000, 0.001)$，则
$$np=16, \sqrt{np(1-p)}\approx \sqrt{16}=4$$
保险公司一年内这项保险收入是：$3\times 16000=48000$ 

（1）、获利不少于10000元，即赔偿不大于 38000(元)，即一年内至多死亡人数为
$$\frac{38000}{2000}=19\text{(人)}$$

所以
$$P\{X\leq 19\}=P\left\{\frac{X-16}{4}\leq \frac{19-16}{4}\right\}\approx\Phi(0.75)=0.7734$$
即该公司获利不少于 10000(元)的概率为 0.7734。

（2）、公司亏本即赔款大于48000元，即一年内至少死亡人数为
$$\frac{48000}{2000}=24\text{(人)}$$ 
所以
\begin{equation*}
\begin{split}
P\{X>24\}&=1-P\{X\leq 24\}=P\left\{\frac{X-16}{4}\leq \frac{24-16}{4}\right\}\\
&\approx 1-\Phi(2)=1-0.97725=0.02275
\end{split}
\end{equation*}
即该公司亏本的概率为0.02275。


